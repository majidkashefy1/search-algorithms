# ğŸ”¢ Sorting Algorithms (Beginner Friendly Guide)

Sorting algorithms are methods for **arranging data in a specific order**, such as ascending or descending. Theyâ€™re fundamental in computer science and appear everywhere â€” from organizing your contact list to optimizing database queries.

---

## ğŸ“˜ Overview

Sorting makes searching and analyzing data faster and easier. There are many sorting algorithms, each with trade-offs in **speed, memory use, and complexity**.

This guide covers five foundational algorithms:

1. **Bubble Sort** ğŸ«§
2. **Insertion Sort** ğŸ§©
3. **Selection Sort** ğŸ¯
4. **Merge Sort** ğŸ§µ
5. **Quick Sort** âš¡

---

## 1ï¸âƒ£ Bubble Sort

**Concept:**
Compares adjacent elements and swaps them if theyâ€™re in the wrong order â€” repeatedly â€” until the list is sorted.

**Real-Life Analogy:**
Imagine youâ€™re arranging books on a shelf by height. You keep comparing pairs and swapping them if one is taller than the other. Eventually, the biggest book â€œbubblesâ€ to the rightmost position.

**Steps:**

1. Compare the first two elements.
2. If theyâ€™re in the wrong order, swap them.
3. Move to the next pair.
4. Repeat until the list is sorted.

**Pseudocode:**

```
for i from 0 to n-1:
    for j from 0 to n-i-2:
        if array[j] > array[j+1]:
            swap(array[j], array[j+1])
```

**Time Complexity:** O(nÂ²)
**Space Complexity:** O(1)
**Pros:** Simple to implement.
**Cons:** Very slow for large datasets.
**When to Use:** For educational purposes or very small lists.

---

## 2ï¸âƒ£ Insertion Sort

**Concept:**
Builds the final sorted array one element at a time by inserting each new element into its proper position.

**Real-Life Analogy:**
Like sorting playing cards in your hand â€” you pick up a new card and place it where it belongs among the cards you already sorted.

**Steps:**

1. Assume the first element is sorted.
2. Take the next element and compare backward.
3. Insert it in the correct position.
4. Repeat for all elements.

**Pseudocode:**

```
for i from 1 to n-1:
    key = array[i]
    j = i - 1
    while j >= 0 and array[j] > key:
        array[j+1] = array[j]
        j = j - 1
    array[j+1] = key
```

**Time Complexity:** O(nÂ²)
**Space Complexity:** O(1)
**Pros:** Efficient for small or nearly sorted data.
**Cons:** Inefficient for large lists.
**When to Use:** When you expect small, mostly sorted data (like adding new entries to a sorted list).

---

## 3ï¸âƒ£ Selection Sort

**Concept:**
Selects the smallest (or largest) element from the unsorted part and places it at the correct position.

**Real-Life Analogy:**
Suppose youâ€™re arranging marbles from smallest to largest. You repeatedly find the smallest one and move it to the beginning.

**Steps:**

1. Find the smallest element.
2. Swap it with the first element.
3. Move the boundary between sorted and unsorted.
4. Repeat until fully sorted.

**Pseudocode:**

```
for i from 0 to n-1:
    min_index = i
    for j from i+1 to n:
        if array[j] < array[min_index]:
            min_index = j
    swap(array[i], array[min_index])
```

**Time Complexity:** O(nÂ²)
**Space Complexity:** O(1)
**Pros:** Simple and performs fewer swaps.
**Cons:** Still inefficient on large data.
**When to Use:** When memory writes are costly and you want fewer swaps.

---

## 4ï¸âƒ£ Merge Sort

**Concept:**
Divides the array into halves, sorts each half, then merges them back together in order.

**Real-Life Analogy:**
Think of organizing two stacks of sorted papers â€” you merge them by picking the smallest top paper from either stack.

**Steps:**

1. Divide the array into halves recursively.
2. Sort each half.
3. Merge the two sorted halves.

**Pseudocode:**

```
function merge_sort(array):
    if length(array) > 1:
        mid = length(array) // 2
        left = array[:mid]
        right = array[mid:]
        merge_sort(left)
        merge_sort(right)
        merge(left, right, array)
```

**Time Complexity:** O(n log n)
**Space Complexity:** O(n)
**Pros:** Very efficient and stable.
**Cons:** Uses more memory.
**When to Use:** For large datasets when stability matters (e.g., sorting records by multiple fields).

---

## 5ï¸âƒ£ Quick Sort

**Concept:**
Selects a pivot element and partitions the array into two halves â€” smaller and larger â€” then recursively sorts each half.

**Real-Life Analogy:**
Imagine dividing books into two piles â€” those shorter and those taller than a chosen one. Then repeat the process for each pile.

**Steps:**

1. Choose a pivot.
2. Rearrange elements: smaller on left, larger on right.
3. Recursively sort the subarrays.

**Pseudocode:**

```
function quick_sort(array):
    if length(array) <= 1:
        return array
    pivot = array[last]
    left = [x for x in array if x < pivot]
    right = [x for x in array if x > pivot]
    return quick_sort(left) + [pivot] + quick_sort(right)
```

**Time Complexity:** O(n log n) (average), O(nÂ²) (worst)
**Space Complexity:** O(log n)
**Pros:** Very fast on average.
**Cons:** Performance drops with poor pivot choices.
**When to Use:** Great general-purpose sorter for large in-memory datasets.

---

## âš–ï¸ Comparison Table

| Algorithm      | Best Case  | Average Case | Worst Case | Space    | Stable | Notes                 |
| -------------- | ---------- | ------------ | ---------- | -------- | ------ | --------------------- |
| Bubble Sort    | O(n)       | O(nÂ²)        | O(nÂ²)      | O(1)     | âœ…      | Good for learning     |
| Insertion Sort | O(n)       | O(nÂ²)        | O(nÂ²)      | O(1)     | âœ…      | Best for small lists  |
| Selection Sort | O(nÂ²)      | O(nÂ²)        | O(nÂ²)      | O(1)     | âŒ      | Few swaps             |
| Merge Sort     | O(n log n) | O(n log n)   | O(n log n) | O(n)     | âœ…      | Stable & efficient    |
| Quick Sort     | O(n log n) | O(n log n)   | O(nÂ²)      | O(log n) | âŒ      | Very fast in practice |

---

## ğŸ§© Next Step

Continue learning with:

* ğŸ” [Search Algorithms](../searches/README_searches.md)
* ğŸŒ [Graph Algorithms](../graphs/README.md)
* âœï¸ [String Algorithms](../strings/README_strings.md)
